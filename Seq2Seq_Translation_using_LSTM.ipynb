{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT95Bn7spXaZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input,LSTM,Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OtfxlHOphYJ",
        "outputId": "ddca8bd8-a756-4407-f603-de65fe6a3c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/data.zip\n",
            "replace __about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace hindi.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ],
      "source": [
        "!unzip /content/data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NZgQvwEphQD"
      },
      "outputs": [],
      "source": [
        "batch_size=128\n",
        "epochs=300\n",
        "latent_dim=64\n",
        "num_samples=900"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIWR20tKphOO"
      },
      "outputs": [],
      "source": [
        "data_path='/content/hindi.txt'\n",
        "\n",
        "# Vectorize the data.\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-RSdU1MphL_",
        "outputId": "83942f23-f072-40f9-b9f1-ecdb0dfc8e92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\tवाह!\\n', '\\tबचाओ!\\n', '\\tउछलो.\\n', '\\tकूदो.\\n', '\\tछलांग.\\n', '\\tनमस्ते।\\n', '\\tनमस्कार।\\n', '\\tवाह-वाह!\\n', '\\tचियर्स!\\n', '\\tसमझे कि नहीं?\\n', '\\tमैं ठीक हूँ।\\n', '\\tबहुत बढ़िया!\\n', '\\tअंदर आ जाओ।\\n', '\\tबाहर निकल जाओ!\\n', '\\tचले जाओ!\\n', '\\tख़ुदा हाफ़िज़।\\n', '\\tउत्तम!\\n', '\\tसही!\\n', '\\tआपका स्वागत है।\\n', '\\tस्वागतम्।\\n', '\\tमज़े करना।\\n', '\\tमौज करना।\\n', '\\tमज़े करो।\\n', '\\tमैं भूल गया।\\n', '\\tमैं भूल गई।\\n', '\\tमैं पैसे दूंगा।\\n', '\\tमैं ठीक हूँ।\\n', '\\tमेरा पेट भर गया है।\\n', '\\tचलो चलें!\\n', '\\tमुझे जवाब दो।\\n', '\\tपंछी उड़ते हैं।\\n', '\\tमाफ़ कीजिए।\\n', '\\tबहुत ख़ूब!\\n', '\\tमैं बेहोश हो गया।\\n', '\\tखेद की बात है, लेकिन वैसा ही है।\\n', '\\tमैं हँसा।\\n', '\\tमैं बोर हो रहा हूँ।\\n', '\\tमेरा दीवालिया हो चुका है।\\n', '\\tमैं थक गया हूँ।\\n', '\\tठंड हो रही है।\\n', '\\tशाबाश!\\n', '\\tकौन जाने?\\n', '\\tकिसको पता है?\\n', '\\tकिसे पता है?\\n', '\\tकिसे मालूम है?\\n', '\\tअद्भुत\\n', '\\tपंछी गाते हैं।\\n', '\\tअंदर आ जाओ।\\n', '\\tनिश्चित ही\\n', '\\tहिलो मत।\\n', '\\tआग जलाती है।\\n', '\\tउसका पीछा करो।\\n', '\\tमुझे तैरना आता है।\\n', '\\tमैं तैर सकता हूँ।\\n', '\\tमैं तुमसे प्यार करती हूँ।\\n', '\\tमैं तुमसे प्यार करता हूँ।\\n', '\\tमैं आपसे प्यार करता हूँ।\\n', '\\tमैं आपसे प्यार करती हूँ।\\n', '\\tमुझे तुमसे प्यार है।\\n', '\\tमैं कोशिश करूँगा।\\n', '\\tमैं आ रहा हूँ।\\n', '\\tमुझे भूख लगी है!\\n', '\\tमुझे भूख लगी है।\\n', '\\tउसे अंदर भेजो।\\n', '\\tउसे अंगर आने दो।\\n', '\\tमुझे बाहर जाने दो!\\n', '\\tऔर एक बार।\\n', '\\tबैठिए।\\n', '\\tशाबाश!\\n', '\\tक्या नया है?\\n', '\\tनया क्या है?\\n', '\\tवह कौन है?\\n', '\\tचिल्लाओ मत!\\n', '\\tचिल्लाईए मत।\\n', '\\tवह खड़ा हो गया।\\n', '\\tवह ताकतवर है।\\n', '\\tआप कैसे हैं?\\n', '\\tतुम कैसे हो?\\n', '\\tतुम कैसी हो?\\n', '\\tतू कैसा है?\\n', '\\tतू कैसी है?\\n', '\\tआप कैसी हैं?\\n', '\\tआप कैसे हो?\\n', '\\tमुझे दोनो पसंद हैं।\\n', '\\tमुझे केक अच्छा लगता है।\\n', '\\tमुझे कुत्ते अच्छे लगते हैं।\\n', '\\tमुझे गणित पसंद है।\\n', '\\tमैं आऊंगा।\\n', '\\tकोई नहीं आया।\\n', '\\tक्या मैं ग़लत था?\\n', '\\tयह क्या है?\\n', '\\tक्या तुम बीमार हो?\\n', '\\tउसको अंदर ले आओ।\\n', '\\tहमारे साथ आओ।\\n', '\\tएसटर मुबारक हो!\\n', '\\tटॉम चला गया क्या?\\n', '\\tमैं घर पर हूँ।\\n', '\\tमैं हिल नहीं सकता।\\n', '\\tमुझे नहीं पता।\\n', '\\tमुझे नहीं मालूम।\\n', '\\tमेरे पास एक गाड़ी है।\\n', '\\tमेरे पास एक कुत्ता है।\\n', '\\tमैं समझता हूँ।\\n', '\\tमैं डॉक्टर हूँ।\\n', '\\tयह किताब है।\\n', '\\tबरफ़ गिर रही है।\\n', '\\tबहुत ज़्यादा बड़ा है।\\n', '\\tकृपया जाईये।\\n', '\\tअविश्वसनीय!\\n', '\\tहम खुश हैं।\\n', '\\tयह क्या है?\\n', '\\tथक गए हो क्या?\\n', '\\tतुम गाड़ी चला सकते हो क्या?\\n', '\\tमोटे मत हो जाना।\\n', '\\tहार मत मान लो।\\n', '\\tपीकर खतम करदो।\\n', '\\tमौत तो सभी की होती है।\\n', '\\tफूल खिलते हैं।\\n', '\\tमैं हूँ जो हूँ।\\n', '\\tमैं उसे लेलूँगा।\\n', '\\tमैं अब थक गया हूँ।\\n', '\\tमैं बहुत व्यस्थ हूँ।\\n', '\\tवह बिल्ली है क्या?\\n', '\\tयह मुफ़्त है।\\n', '\\tयह मुफ़्त का है।\\n', '\\tमुझे दिखाओ।\\n', '\\tमुझे देखने दो।\\n', '\\tमुझे ट्राए करने दो।\\n', '\\tजल्दी कीजिए।\\n', '\\tमैं अंदर आ सकता हूँ क्या?\\n', '\\tदरवाज़ा खोलो।\\n', '\\tदरवाज़ा खोलिए।\\n', '\\tकृपया अंदर आईए।\\n', '\\tइसे दोबारा पढ़ें।\\n', '\\tपढ़के सुनाओ।\\n', '\\tवह नीचे झुकी।\\n', '\\tकुछ मछलियाँ उड़ सकतीं हैं।\\n', '\\tयह नक्शा है।\\n', '\\tटॉम मेरा लड़का है।\\n', '\\tहम शहर में हैं।\\n', '\\tक्या तुम्हें गोली लग गई थी?\\n', '\\tहमारा क्या?\\n', '\\tक्या मैं आपकी सहायता कर सकता हूँ?\\n', '\\tक्या मैं आपकी मदद कर सकता हूँ?\\n', '\\tकमरे को साफ़ करो।\\n', '\\tउसे छूना मत।\\n', '\\tबिस्तर से बाहर निकलो!\\n', '\\tनव वर्ष की शुभकामनाएं!\\n', '\\tनए साल की बधाईयाँ।\\n', '\\tजन्मदिन मुबारक हो!\\n', '\\tउसके पास दाढ़ी है।\\n', '\\tवह अभिनेता है।\\n', '\\tउसे पैसों की ज़रूरत है।\\n', '\\tउसको पैसों की कमी थी।\\n', '\\tमुझे इतिहास पसंद है।\\n', '\\tमुझे यह कुत्ता अच्छा लगता है।\\n', '\\tमुझे एक तो खरीदना ही होगा।\\n', '\\tमैं वापस आऊँगा।\\n', '\\tमैं तुम्हें फ़ोन करूँगा।\\n', '\\tमैं घर पर ही रहूँगा।\\n', '\\tमैं अनीश्वरवादी हूँ।\\n', '\\tमैं भगवान में यकीन नहीं करता।\\n', '\\tमैं बहुत थक गया हूँ।\\n', '\\tआज मौसम बहुत गरम है।\\n', '\\tयह किताब है।\\n', '\\tतुम्हारी चाल है।\\n', '\\tभगवान जाने।\\n', '\\tगर्मियाँ खतम हों चुकीं हैं।\\n', '\\tआराम से आओ।\\n', '\\tसोच लो।\\n', '\\tयह एक किताब है।\\n', '\\tयह किताब है।\\n', '\\tयह मेरा बस्ता है।\\n', '\\tयह मेरा कुत्ता है।\\n', '\\tअपने पैर धोओ।\\n', '\\tहम संगीत की पढ़ाई करते हैं।\\n', '\\tहम आपको फ़ोन करेंगे।\\n', '\\tहम आपको बुलाएँगे।\\n', '\\tतुम कहाँ थे?\\n', '\\tआप कहाँ थे?\\n', '\\tतुम्हें जाना होगा।\\n', '\\tतुम थके-हारे से लगते हो।\\n', '\\tमज़ाक कर रहे हो!\\n', '\\tएक आदमी के लिए काम करना ज़रूरी है।\\n', '\\tतुम घर पे हो क्या?\\n', '\\tक्या आप लोग ठीक हो?\\n', '\\tजल्दी घर आजाओ।\\n', '\\tआ सको तो आना।\\n', '\\tबधाई हो!\\n', '\\tमुबारक हो!\\n', '\\tवह वहाँ गया क्या?\\n', '\\tमेरी याद आई क्या?\\n', '\\tपागल मत बनो।\\n', '\\tवह भागते हुए आया।\\n', '\\tवह बीमार नहीं हो सकता।\\n', '\\tउसने टोपी पहनी हुई है।\\n', '\\tवह अरबी पढ़ सकता है।\\n', '\\tयह मेरा पति है।\\n', '\\tसब लोग कैसे हैं?\\n', '\\tकैसा चल रहा है?\\n', '\\tतुम कितने साल के हो?\\n', '\\tकितने बदतमीज़ हो तुम!\\n', '\\tबदतमीज़!\\n', '\\tमैंने रात का खाना पकाया।\\n', '\\tमुझे उल्टी आ रही है।\\n', '\\tमैं हर साल जाता हूँ।\\n', '\\tमुझे कोई आवाज़ सुनाई दी।\\n', '\\tमुझे यह वाला पसंद है।\\n', '\\tमैं रोज़ दौड़ने जाता हूँ।\\n', '\\tमैंने उसे फिरसे देखा।\\n', '\\tमुझे गिटार चाहिए।\\n', '\\tमुझपर हमला किया गया।\\n', '\\tमैं अच्छा रसोइया हूँ।\\n', '\\tमैं अच्छा बावर्ची हूँ।\\n', '\\tआज मेरा नसीब अच्छा है।\\n', '\\tकोई है क्या?\\n', '\\tक्या यह काला नहीं है?\\n', '\\tयहाँ कैफ़े है क्या?\\n', '\\tचलो पतंग उड़ाएँ!\\n', '\\tनहीं, मैं नहीं गया था।\\n', '\\tआसपास कोई नहीं है।\\n', '\\tबस अब रोना बंद करो।\\n', '\\tअपना मूँह खोलो!\\n', '\\tबैठिए।\\n', '\\tवह तो हसीना है।\\n', '\\tवह बहुत बोलती है।\\n', '\\tवह बहुत बक-बक करती है।\\n', '\\tयह तो ठीक नहीं है।\\n', '\\tवह मेरी गलती है।\\n', '\\tकुत्ता मेरा है।\\n', '\\tबिजली गई हुई है।\\n', '\\tये चिड़ियाँ हैं।\\n', '\\tयह तुम्हारे लिए है।\\n', '\\tहमे किसी भी चीज़ की कमी नहीं है।\\n', '\\tअभी कितने बजे हैं?\\n', '\\tअभी क्या समय हो रहा है?\\n', '\\tमेरी किताब कहाँ है?\\n', '\\tतुमने मुझे डरा दिया!\\n', '\\tतुम इनसान हो।\\n', '\\tआप डॉक्टर हैं क्या?\\n', '\\tक्या तुम मुझे सिखा सकते हो?\\n', '\\tऊपर तक चढ़ जाओ।\\n', '\\tआओ हमारे साथ बैठो।\\n', '\\tआओ हमारे साथ बैठो।\\n', '\\tआलूओं को काटो।\\n', '\\tतुम्हारे पास चावल है क्या?\\n', '\\tमूँह मत बनाओ।\\n', '\\tशोर मत मचाओ।\\n', '\\tमज़े करो।\\n', '\\tमज़े करो।\\n', '\\tउसके पैर लम्बे हैं।\\n', '\\tवह उसका दोस्त है।\\n', '\\tउसे संत्रे अच्छे लगते हैं।\\n', '\\tउसे नारंगी अच्छे लगते हैं।\\n', '\\tवह तुमसे प्यार करता होगा।\\n', '\\tवह अरबी बोलता है।\\n', '\\tयह लो, तुम्हारा बस्ता।\\n', '\\tमुझे याद नहीं।\\n', '\\tमैं उसे नहीं जानता।\\n', '\\tमैं मुश्किल में हूँ।\\n', '\\tमुझे अभी जाना है।\\n', '\\tमैं आस-पड़ोस में रहता हूँ।\\n', '\\tमैं अपनी माँ से प्यार करता हूँ\\n', '\\tमुझे अब जाना होगा।\\n', '\\tमैं बहुत थक गया हूँ।\\n', '\\tमुझे बहुत प्यास लगी है।\\n', '\\tआज एक अप्रैल है।\\n', '\\tअब मेरी बारी है।\\n', '\\tमै फिर से कोशिश करता हूँ।\\n', '\\tआपसे मिलकर खुशी हुई।\\n', '\\tमेरे साथ आईए।\\n', '\\tयहाँ अपना हस्ताक्षर कीजिए।\\n', '\\tउसने तुम्हें धोखा दिया।\\n', '\\tबैठिए।\\n', '\\tगाड़ी तैयार है।\\n', '\\tयह तुम्हारा कुत्ता है।\\n', '\\tयह आपकी चाबी है।\\n', '\\tयह सही नहीं है।\\n', '\\tटॉम मेरा दोस्त है।\\n', '\\tहम घर जा रहें हैं।\\n', '\\tहम जल्दी में हैं।\\n', '\\tतुम्हारी उम्र क्या है?\\n', '\\tवह कौनसी चिड़िया है?\\n', '\\tक्या कहानी है?\\n', '\\tआपका नाम क्या है?\\n', '\\tतुम कहाँ गए थे?\\n', '\\tशराबख़ाना कहाँ है?\\n', '\\tकिसकी बारी है?\\n', '\\tतुम शायद सही हो।\\n', '\\tतुम्हारी आँखे खराब हैं क्या?\\n', '\\tपक्षी घोसले बनातें हैं।\\n', '\\tचिड़ियाँ घोसले बनातीं हैं।\\n', '\\tलड़के तो लड़के ही रहेंगे।\\n', '\\tमैं कुछ कर सकता हूँ क्या?\\n', '\\tआप बाहर नहीं गए थें क्या?\\n', '\\tतुम बाहर नहीं गए थे क्या?\\n', '\\tतू बाहर नहीं गया था क्या?\\n', '\\tक्या तुम्हे मुझपर यकीन है?\\n', '\\tक्या तुम मुझपर यकीन करते हो?\\n', '\\tक्या तुम मेरा यकीन करते हो?\\n', '\\tउसे धोका मत दो।\\n', '\\tवह चिल्लाने लगा।\\n', '\\tवह मुझसे मिलने आया था।\\n', '\\tवह नौकरानी रखता है।\\n', '\\tवह किसी नौकरी के पीछे पड़ा है।\\n', '\\tवह अब चल रहा है।\\n', '\\tउसने विदेश में पढ़ाई करी थी।\\n', '\\tवह देशद्रोही बन गया।\\n', '\\tवह जाचुका है।\\n', '\\tमैं गाड़ी चला सकता हूँ।\\n', '\\tमैं पूरी रात रोई।\\n', '\\tमुझे नींद आ गई।\\n', '\\tमेरे सिर में दर्द हो रहा है।\\n', '\\tमुझे सिरदर्द हो रहा है।\\n', '\\tमेरे पास कुछ पैसे हैं।\\n', '\\tमेरी उससे मुलाकात नहीं हुई है।\\n', '\\tमैं संगीत सुनता हूँ।\\n', '\\tमैंने उसे दौड़ते हुए देखा।\\n', '\\tमैंने उसे भागते हुए देखा।\\n', '\\tमैं उससे सच्चा प्यार करता था।\\n', '\\tमैं बैंक में काम करता हूँ।\\n', '\\tमैं खुश हो रहा हूँ।\\n', '\\tमैं विद्यार्थी नहीं हूँ।\\n', '\\tमैं वैसा नहीं हूँ।\\n', '\\tमुझे बहुत भूख लगी है।\\n', '\\tमेरी ग़लती नहीं है।\\n', '\\tशुरू हो रहा है।\\n', '\\tमेरा बस्ता चोरी हो गया था।\\n', '\\tमेरी आँखें थक गईं हैं।\\n', '\\tआसपास कोई नहीं है।\\n', '\\tहाथ ऊपर करो!\\n', '\\tवह गाने लगी।\\n', '\\tउसकी नीली आँखें हैं।\\n', '\\tवह शादीशुदा नहीं है।\\n', '\\tमुझे सच्चाई बताओ।\\n', '\\tपृथ्वी घूमती है।\\n', '\\tएक बार फिरसे कोशिश करो।\\n', '\\tएक बार और प्रयत्न करो।\\n', '\\tरेडियो को चालू करो।\\n', '\\tहम उसे नहीं जानते हैं।\\n', '\\tहम उसे नहीं जानते हैं।\\n', '\\tहमारे पास खुशखबर है।\\n', '\\tहम जंग के ख़िलाफ़ हैं।\\n', '\\tहम युद्ध का विरोध करते हैं।\\n', '\\tखुशी क्या होती है?\\n', '\\tसुख क्या होता है?\\n', '\\tआपका नाम क्या है?\\n', '\\tमुश्किल क्या है?\\n', '\\tआप कहाँ रहते हैं?\\n', '\\tतुम कहाँ रहते हो?\\n', '\\tतू कहाँ रहता है?\\n', '\\tतुम कहाँ रहती हो?\\n', '\\tतू कहाँ रहती है?\\n', '\\tवह किताब कहाँ है?\\n', '\\tसमुद्र तट कहां है?\\n', '\\tयह पाय किसने बनाई है?\\n', '\\tहाँ। यह सही है।\\n', '\\tतुम्हें जाना होगा।\\n', '\\tतुम आज बिज़ी हो क्या?\\n', '\\tतुम्हारे पास आज समय है क्या?\\n', '\\tहम बात कर सकते हैं क्या?\\n', '\\tहमारे साथ आओ।\\n', '\\tतुम अच्छे से सोये क्या?\\n', '\\tतुम्हें मैं याद हूँ क्या?\\n', '\\tउम्मीद मत छोड़ो।\\n', '\\tपत्थर मत फेंको।\\n', '\\tउसे सब पसंद करते हैं।\\n', '\\tउसे सब पसंद करते हैं।\\n', '\\tतेरी अकल घास चरने गई है क्या?\\n', '\\tउसने गहरी साँस ली।\\n', '\\tउसने अपनी आँखें बंद करीं।\\n', '\\tवह रोया और और रोया।\\n', '\\tवह साधा आदमी है।\\n', '\\tवह ज़रूर आएगा।\\n', '\\tवह मेरा चचेरा भाई नहीं है।\\n', '\\tवह मेरा ममेरा भाई नहीं है।\\n', '\\tउसने अपनी बारी का इंतेज़ार किया।\\n', '\\tवह बैंक में काम करता है।\\n', '\\tवह अच्छा इनसान है।\\n', '\\tमैं अपनी ग़लती मानता हूँ।\\n', '\\tमैं अंग्रेज़ी पढ़ सकता हूँ।\\n', '\\tमैं अंग्रेज़ी पढ़ सकती हूँ।\\n', '\\tमुझे विश्वास नहीं होता!\\n', '\\tमैंने उसके बाहर जाने की आवाज़ सुनी।\\n', '\\tमुझे उसका पता पता है।\\n', '\\tमुझे उसका पता पता है।\\n', '\\tमैं उन औरतों को जानता हूँ।\\n', '\\tमैं तुम्हारे पिताजी को जानता हूँ।\\n', '\\tमुझे वह अच्छा सा लगता है।\\n', '\\tमुझे अभी भी नहीं पता।\\n', '\\tमैं १९६० में पैदा हुआ था।\\n', '\\tमैं १९७९ में पैदा हुआ था।\\n', '\\tमैं टीवी देखता हूँ।\\n', '\\tमैंने पूरी रात काम किया।\\n', '\\tमैंने रातभर काम किया।\\n', '\\tबाद में मिलेंगे।\\n', '\\tफिर मिलेंगे।\\n', '\\tमैं तुम्हारे जितना लम्बा हूँ।\\n', '\\tमैं अभी आता हूँ।\\n', '\\tक्या यह किताब तुम्हारी है?\\n', '\\tअभी से रात हो गई है।\\n', '\\tबरफ़ जैसा दिखता है।\\n', '\\tबरफ़ गिरनी शुरू गो गई।\\n', '\\tवो मजेदार था I\\n', '\\tबहुत महंगा है!\\n', '\\tबहुत महंगी है!\\n', '\\tयहाँ बहुत गरम है।\\n', '\\tयह मुझे संभालने दो।\\n', '\\tरेलगाड़ी से चलते हैं।\\n', '\\tमेरी गाड़ी टोयोटा है।\\n', '\\tमेरा परिवार छोटा है।\\n', '\\tकोई मेरी मदद नहीं कर सकता।\\n', '\\tवह बहुत सुंदर है।\\n', '\\tवह खुशी से उछल पड़ी।\\n', '\\tउसने खुशी से मुस्कुराया।\\n', '\\tवह डॉक्टर नहीं है।\\n', '\\tमुझे उस आदमी से चिढ़ आती है।\\n', '\\tबॅटर आऊट हो गया था।\\n', '\\tकौआ उड़ गया।\\n', '\\tबिजली चली गई।\\n', '\\tये कुत्ते बड़े हैं।\\n', '\\tयह मेरी का कुत्ता है।\\n', '\\tउसकी कहानी सच्ची थी क्या?\\n', '\\tहमे स्केटिंग करके मज़ा आया।\\n', '\\tहम जल्दी में थे।\\n', '\\tहम बहुत थक चुके थे।\\n', '\\tहम अच्छे दोस्त हैं।\\n', '\\tकितना बड़ा कद्दू है!\\n', '\\tमुश्किल क्या है?\\n', '\\tशुरू कब होता है?\\n', '\\tबच्चे कहाँ गए?\\n', '\\tतुम्हारा कुत्ता कौनसा है?\\n', '\\tगुलदान को किसने तोड़ दिया?\\n', '\\tयह केक किसने बनाया है?\\n', '\\tगाड़ी कौन चला रहा है?\\n', '\\tयह किसकी किताब है?\\n', '\\tतुम मेरी कलम इस्तेमाल कर सकते हो।\\n', '\\tमेरे सवालों का जवाब दो।\\n', '\\tवे जिमखाने में हैं क्या?\\n', '\\tतुम्हें गाड़ी चलाना आता है क्या?\\n', '\\tतुम गाड़ी चला सकते हो क्या?\\n', '\\tयहाँ हस्ताक्षर कर सकते हैं क्या?\\n', '\\tउसे सब पसंद करते हैं।\\n', '\\tउसे सब पसंद करते हैं।\\n', '\\tमाफ़ कीजिएगा, मैं खो गई हूँ।\\n', '\\tउसने मेरे लिए टेक्सी बुलवाई।\\n', '\\tउसे बहुत अकेलापन महसूस हुआ।\\n', '\\tवह मेरे पड़ोस में रहता है।\\n', '\\tवह बहुत थका-हारा सा लगता है।\\n', '\\tवह दिखने में बहुत अमीर लगता है।\\n', '\\tवह जल्द ही ठीक हो जाएगा।\\n', '\\tउसकी हालत बुरी है।\\n', '\\tऔर स्वाद कैसा है?\\n', '\\tतुमने कैसे बनाया?\\n', '\\tमैं भारत से नहीं हूँ।\\n', '\\tमैं किताब पढ़ रही हूँ।\\n', '\\tमैंने पुरानी गाड़ी खरीदी।\\n', '\\tमैं गाता चला गया।\\n', '\\tमैं समझा नहीं।\\n', '\\tमुझे गर्मी का मौसम पसंद नहीं है।\\n', '\\tमैं उससे पूछना भूल गया।\\n', '\\tमुझे जोखिम लेना पसंद नहीं है।\\n', '\\tमैंने अभी तक नहीं खाया है।\\n', '\\tमैं दोनो को जानता था।\\n', '\\tमुझे पता है कि वह चीज़ क्या है।\\n', '\\tमैंने तुम्हे बताने की कोशिश करी थी।\\n', '\\tमैं उससे शादी करना चाहता हूँ।\\n', '\\tमैं चाहता था कि वह जीत जाए।\\n', '\\tमुझे शहर का नक्शा चाहिए।\\n', '\\tमैं उसे देखना चाहूँगा।\\n', '\\tमैं उससे मिलना चाहूँगा।\\n', '\\tमैं दस बजे वापस आऊँगा।\\n', '\\tमैं शराब लाता हूँ।\\n', '\\tमैं शराब लाऊँगा।\\n', '\\tमैं तुम्हें खुश करूँगा।\\n', '\\tमैंने फ़ोन कर लिया है।\\n', '\\tमेरी टिकट खो गई है।\\n', '\\tचलो बेसबॉल खेलतें हैं।\\n', '\\tमुझे स्याही से ही लिखना होगा?\\n', '\\tमेरे परिवार को टॉम पसंद था।\\n', '\\tमेरे पैसे चोरी हो गए।\\n', '\\tमेरी बहन के पास नौकरी है।\\n', '\\tवह काम करती रही।\\n', '\\tउसने मेरी कमीज़ खींची।\\n', '\\tवह बाघ नहीं है।\\n', '\\tवह मेरी ग़लती थी।\\n', '\\tआसमान साफ़ है।\\n', '\\tपेड़ हरे हैं।\\n', '\\tये हमारी किताबें हैं।\\n', '\\tयह वही चीज़ है जिसकी मुझे ज़रूरत है।\\n', '\\tज़बान सम्भाल के बात करो।\\n', '\\tहम अच्छे दोस्त हैं।\\n', '\\tहमें टैक्स भरना पड़ेगा।\\n', '\\tतुमने क्या जवाब दिया?\\n', '\\tइस डब्बे में क्या है?\\n', '\\tतुम क्या कर रहे थे?\\n', '\\tतुम कहाँ जा रहे हो?\\n', '\\tआप कहाँ जा रहे हैं?\\n', '\\tहवाई अड्डा कहाँ पर है?\\n', '\\tतुम्हारी किताब कौनसी है?\\n', '\\tतुम्हारे टीचर कौन हैं?\\n', '\\tतुम क्यों नहीं आए?\\n', '\\tतुम हमारे साथ आओगे क्या?\\n', '\\tसर्दी आ रही है।\\n', '\\tतुम फिरसे चालू हो गए।\\n', '\\tतुम्हे मुझे माफ़ करना होगा।\\n', '\\tआपको मुझे क्षमा करना होगा।\\n', '\\tतुम्हें मन लगाकर पढ़ना होगा।\\n', '\\tतुमने मेरी जान बचाली।\\n', '\\tमैंने तुम्हें बताया नहीं था?\\n', '\\tतुम्हारे पास पेनसिल है क्या?\\n', '\\tअपनी किताब मत खोलो।\\n', '\\tमेरी चिंता मत करो।\\n', '\\tहमारी चिंता मत करो।\\n', '\\tमछलियाँ समुंदर में रहतीं हैं।\\n', '\\tआप विदेश गए हुए हैं क्या?\\n', '\\tउसको मेरी कामयाबी से जलन थी।\\n', '\\tवह मुझपर गुस्सा हो गया।\\n', '\\tवह भारत गया हुआ है।\\n', '\\tवह तुमसे नाराज़ है।\\n', '\\tउसको खुश करना बहुत मुश्किल है।\\n', '\\tउसकी हालत बुरी है।\\n', '\\tउसने मुझे दो किताबें उधार दीं।\\n', '\\tउसने कोई बहाना बना लिया।\\n', '\\tउसने पैसों के लिए शादी की।\\n', '\\tउसने एक सुंदर लड़की को देखा।\\n', '\\tवह कामयाब होना चाहता था।\\n', '\\tतुम्हारी नौकरी कैसी चल रही है?\\n', '\\tमैं इस योजना से सहमत हूँ।\\n', '\\tमुझे यहाँ पर अजनबी के जैसा महसूस होता है।\\n', '\\tमैं तुमपर भरोसा कर रहा हूँ।\\n', '\\tमैं सेव खा रहा हूँ ।\\n', '\\tमैं सेव खा रही हूँ ।\\n', '\\tमुझे प्रतीक्षा करने में कोई आपत्ती नहीं है।\\n', '\\tमुझे लगता है वह आएगा।\\n', '\\tमुझे उल्टी आ रही है।\\n', '\\tमेरे पास कुछ दोस्त हैं।\\n', '\\tमुझे पेट में दर्द हो रहा है।\\n', '\\tमुझे पीठ की तकलीफ है I\\n', '\\tमुझे ज़ुकाम हो गया है।\\n', '\\tमैंने अपनी ऊँगली काट ली।\\n', '\\tमुझे पता है उसने क्यों किया था।\\n', '\\tमैं बड़े शहर में रहता हूँ।\\n', '\\tमैं बेहोशी में चला गया।\\n', '\\tमैं बेहोश हो गया।\\n', '\\tमैंने तो मज़ाक के तौर पर कहा था।\\n', '\\tमैंने तो मज़ाक समझकर बोला था।\\n', '\\tमैं कल मेरी से मिला था।\\n', '\\tमेरे ख़याल से तुम सही हो।\\n', '\\tमुझे अपने पैसे वापस चाहिए।\\n', '\\tमैं तुम्हारे साथ आऊँगा।\\n', '\\tमैं मानता हूँ कि मैं ग़लत था।\\n', '\\tमैं कभी वापस नहीं आऊँगा।\\n', '\\tमुझे मछली से एलर्जी है।\\n', '\\tमेरा चश्मा खो गया है।\\n', '\\tमेरे चश्मे खो गए हैं।\\n', '\\tमैं फ़ैसला कर चुका हूँ।\\n', '\\tवह किताब पढ़ रहा है क्या?\\n', '\\tरविवार को खुला होता है क्या?\\n', '\\tफ़ोन बज रहा है क्या?\\n', '\\tग्यारह बज चुके हैं।\\n', '\\tयह मुफ़्त का है।\\n', '\\tयह जीत तो बहुत आसान थी।\\n', '\\tमुझे तुम्हें घर लेजाने दो।\\n', '\\tलिंकन की मौत १८६५ में हुई थी।\\n', '\\tजल्दी कीजिए।\\n', '\\tदरवाज़े को बंद कर दीजिए।\\n', '\\tउसने उसे एक घड़ी दी।\\n', '\\tउसने लाल रंग के कपड़े पहने थे।\\n', '\\tउसने लाल ड्रेस पहनी थी।\\n', '\\tवह उसकी ख़ासियत है।\\n', '\\tइस घर में भूत है।\\n', '\\tघर आग में लिपटा हुआ था।\\n', '\\tउस आदमी ने मेरी ओर देखा।\\n', '\\tचादरें गीलीं लगतीं हैं।\\n', '\\tपानी का पाइप फट गया।\\n', '\\tये किताबें मेरीं हैं।\\n', '\\tयह जूते उसके हैं।\\n', '\\tवे ज़ू गए थे।\\n', '\\tयह रेशम जैसा लगता है।\\n', '\\tयह घड़ी टूटी हुई है।\\n', '\\tसमय तेज़ी से बीतता है।\\n', '\\tअपने हाथ अच्छे से धोओ।\\n', '\\tआज कौनसा वार है?\\n', '\\tइसमें क्या है?\\n', '\\tतुम कब लौट कर आओगे?\\n', '\\tमुश्किल क्या है?\\n', '\\tमुश्किल कहाँ है?\\n', '\\tतुम्हारे पिता कहाँ हैं?\\n', '\\tतुम रेलगाड़ी से जाओगे क्या?\\n', '\\tतुम वह साबित नहीं कर सकते।\\n', '\\tतुम आज खुश लगते हो।\\n', '\\tतुम्हारा कुत्ता बहुत बड़ा है।\\n', '\\tतुम्हारा कुत्ता बहुत मोटा है।\\n', '\\tमैं तुम्हारी पेनसिल इस्तेमाल कर सकता हूँ क्या?\\n', '\\tक्या यह अफ़वाह सच हो सकती है?\\n', '\\tतुम राज़ रख सकते हो क्या?\\n', '\\tतुमने मेरा कैमरा देखा क्या?\\n', '\\tक्या तुम भगवान में विश्वास करते हो?\\n', '\\tक्या आपको अपना नाप पता है?\\n', '\\tक्या तुम्हें यह किताब पसंद है ?\\n', '\\tवह अंग्रेज़ी बोलता है क्या?\\n', '\\tनिराश मत हो।\\n', '\\tमोटी मुर्गियाँ कम अंडे देतीं हैं।\\n', '\\tघंटी बज चुकी है क्या?\\n', '\\tउसने हमारा प्रस्ताव स्वीकार कर लिया।\\n', '\\tवह पुलिस अफ़सर बन गया।\\n', '\\tवह पुलिसवाला बन गया।\\n', '\\tवह सीड़ियों पर चढ़ गया।\\n', '\\tवो फ़र्निचर का व्यापार करता है।\\n', '\\tउसे बहुत सारा पैसा मिला।\\n', '\\tउसके पास थोड़े-बहुत पैसे हैं।\\n', '\\tवह बहुत पैसेवाला है।\\n', '\\tउसका निशाना बहुत अच्छा है।\\n', '\\tवह गरीब है, पर खुश है।\\n', '\\tवह लम्बा और ताकतवर है।\\n', '\\tवह हमको अच्छी तरह से जानता है।\\n', '\\tवह दिखने में घोड़े की तरह लगता है।\\n', '\\tवे कम-से-कम साठ बरस के तो होंगे।\\n', '\\tउसने गाते गाते काम किया।\\n', '\\tवह मेरा अच्छा दोस्त था।\\n', '\\tवह पिछले हफ़्ते बीमार था।\\n', '\\tउसने प्रतियोगिता फिरसे जीत ली।\\n', '\\tवह सुंदर लिखता है।\\n', '\\tतुम्हे आज कैसा लग रहा है?\\n', '\\tमैं उनका आभारी हूँ।\\n', '\\tमुझे ढंग से तैरना नहीं आता।\\n', '\\tमैं अपने काम से थक चुका हूँ।\\n', '\\tआपको जाने नहीं दे सकता हूँ।\\n', '\\tमैंने नदी में छलाँग लगाकर डुबकी ली।\\n', '\\tमैं भी बॉस्टन में ही रहता हूँ।\\n', '\\tमैंने उसे अपना नौकर बना लिया।\\n', '\\tमुझे जीव विज्ञान कभी भी पसंद नहीं था।\\n', '\\tमैं हड़बड़ी में दौड़ गया।\\n', '\\tमैं कोरिया जाना चाहता हूँ।\\n', '\\tमैं इतना थक गया हूँ कि मुझसे और चला नहीं जाएगा।\\n', '\\tमैं फ़ैसला कर चुका हूँ।\\n', '\\tक्या मैरी तुम्हारी बेटी है?\\n', '\\tतुम्हारी घड़ी सही है क्या?\\n', '\\tकल बरफ़ पड़ेगी।\\n', '\\tपार्टी शुरू करते हैं।\\n', '\\tइसको अपना घर ही समझो।\\n', '\\tमई अप्रैल के बाद आता है।\\n', '\\tमेरी गाड़ी चल नहीं रही है।\\n', '\\tमेरी आँख सूज गई है।\\n', '\\tसारे पंछी उड़ नहीं सकते।\\n', '\\tदरवाज़े को बंद कर दीजिए।\\n', '\\tमुझे अकेला छोड़ दीजिए।\\n', '\\tगेंद फेंकिए।\\n', '\\tजहाँ भी डालना है डालदो।\\n', '\\tजहाँ भी रखना है रखदो।\\n', '\\tजहाँ भी घुसाना है घुसा दो।\\n', '\\tवह हमेशा मन लगाकर काम करती है।\\n', '\\tउसने मुझसे मदद मांगी।\\n', '\\tवह मेरे लिए एक नन्हा सा खिलौना खरीद लाई।\\n', '\\tवह अपने पति से नफ़रत करती थी।\\n', '\\tवह पाँच साल की है।\\n', '\\tउसने लम्बी ज़िन्दगी जी।\\n', '\\tवह एक ज़िद्दी लड़की है।\\n', '\\tकहीं भी बैठ जाओ।\\n', '\\tमुझे बताओ क्या हुआ।\\n', '\\tझील यहाँ पर बहुत गहरी है।\\n', '\\tउस आदमी ने सारी उम्मीद छोड़दी।\\n', '\\tस्टेशन पास में है।\\n', '\\tवे एक-दूसरे से नफ़रत करते थे।\\n', '\\tउन्होंने मेरी का मज़ाक उड़ाया।\\n', '\\tउनकी ट्रेन छूट गई।\\n', '\\tउन्होंने मदद के लिए चिल्लाया।\\n', '\\tउन्होंने मदद के लिए आवाज़ लगाई।\\n', '\\tयह भी सेव है।\\n', '\\tटॉम और मैं दोस्त हैं।\\n', '\\tहमने उसके प्रस्ताव को स्वीकार कर लिया।\\n', '\\tहम उनको परेशान कर रहें हैं।\\n', '\\tहमने थोड़ी देर आराम किया।\\n', '\\tमुझे चलना चाहिए।\\n', '\\tयह क्या बकवास है?\\n', '\\tतुम दोनो क्या कर रहे हो?\\n', '\\tतुम दोनो क्या कर रहे थे?\\n', '\\tलिफ़्ट कहाँ पर है?\\n', '\\tये क्या किताबें हैं?\\n', '\\tवह कल आएगा क्या?\\n', '\\tवे कल आएँगे क्या?\\n', '\\tतुम समझे नहीं।\\n', '\\tजैसी करनी वैसी भरनी।\\n', '\\tजैसा बोओगे, वैसा काटोगे।\\n', '\\tअच्छा करो, अच्छा पाओ।\\n', '\\tकर भला तो हो भला।\\n', '\\tजैसा करना वैसा भरना।\\n', '\\tतुम्हारे बाल कुछ ज़्यादा ही लम्बे हैं।\\n', '\\t\"कौन है?\" \"मैं हूँ\"\\n', '\\tलोमड़ी जंगली जानवर होती है।\\n', '\\tखरगोश के कान लम्बे होते हैं।\\n', '\\tक्या कोई तुम्हारा यकीन कर सकता है?\\n', '\\tतुमने पूरा पढ़ लिया क्या?\\n', '\\tयह रंग पसंद है क्या?\\n', '\\tमुझसे पैसे मत माँगो।\\n', '\\tनाराज़ मत हो जाओ।\\n', '\\tउनका मज़ाक मत उड़ाओ।\\n', '\\tमुझे तुम्हें थप्पड़ करने के लिए मजबूर मत करो।\\n', '\\tऐसी बात मत बोलो।\\n', '\\tवह काम शुरु मत करो।\\n', '\\tवह गाना सभी को पता था।\\n', '\\tमछलियाँ पानी में जीतीं हैं।\\n', '\\tवह ज़्यादा तेज़ नहीं भाग सकता।\\n', '\\tवह अच्छा कमा लेता है।\\n', '\\tउसकी तीन बेटियाँ हैं।\\n', '\\tउसने दरवाज़े पर खटखटाया।\\n', '\\tउसने तैरना सीखा।\\n', '\\tउसने मेरा बस्ता चुरा लिया।\\n', '\\tउसने मुझे बताया कहाँ जाना है।\\n', '\\tउसने किताब को फाड़ डाला।\\n', '\\tवह भौतिक विज्ञान समझता है।\\n', '\\tवह बस अभी आता होगा।\\n', '\\tवह बस आता ही होगा।\\n', '\\tवह थोड़ी देर में वापस आ रहा है।\\n', '\\tउसके पिता जापानी हैं।\\n', '\\tमैंने उससे वह करने को कहा था।\\n', '\\tमैं तुमसे सहमत नहीं हूँ।\\n', '\\tमैं भगवान में यकीन नहीं करता।\\n', '\\tमुझे तुम्हारी दया नहीं चहिए।\\n', '\\tमुझे पिंजरा खाली मिला।\\n', '\\tमैं स्कूल चल कर जाती हूँ।\\n', '\\tमैंने उस औरत को गस्सा दिलाया।\\n', '\\tमुझे चाय से ज़्यादा कॉफ़ी पसंद है।\\n', '\\tमुझे बुखार जैसा लगता है।\\n', '\\tमुझे अभी भी नहीं पता।\\n', '\\tमैं सात बजे सोकर उठता हूँ।\\n', '\\tमैं अपने-आप करना चाहता हूँ।\\n', '\\tमुझे नौकरी से निकाला नहीं गया। मैंने खुद ही नौकरी छोड़ दी।\\n', '\\tमैं नौ बजे से पहले वापस आ जाऊँगा।\\n', '\\tपता नहीं क्या हुआ।\\n', '\\tमुझे एक ब्लडी मेरी चाहिए।\\n', '\\tमैं फ़ोन करता हूँ।\\n', '\\tमैं अभी हवाईअड्डे में हूँ।\\n', '\\tमैं तुम्हारे बारे में सोच रहा हूँ।\\n', '\\tक्या आज कोई अनुपस्थित है?\\n', '\\tआज कोई है जो नहीं आया है?\\n', '\\tवह मेरे लिए मुश्किल है।\\n', '\\tआज मौसम काफ़ी ठंडा है।\\n', '\\tअपने काम से काम रखो!\\n', '\\tमेरे पिता बहुत अच्छे हैं।\\n', '\\tमेरे पापा बहुत अच्छे हैं।\\n', '\\tमेरा पेट गुड़गुड़ा रहा है।\\n', '\\tन्यू यॉर्क बड़ा शहर है।\\n', '\\tस्कूल अप्रैल में शुरू होता है।\\n', '\\tस्कूल साढ़े तीन बजे छूटता है।\\n', '\\tमैं कमरे की सफ़ाई करूँ क्या?\\n', '\\tवह हमेशा काले कपड़े पहनती है।\\n', '\\tवह पियानो बजाना जानती है।\\n', '\\tवह नर्म दिल वाली है।\\n', '\\tवह बहुत पैसेवाली औरत है।\\n', '\\tवह गरीब है, पर खुश है।\\n', '\\tउसने पैसेवाले से शादी की।\\n', '\\tवह बहुत बक-बक करती है।\\n', '\\tउसने मुझे अपना कमरा दिखाया।\\n', '\\tक्या मैं कमरा साफ़ कर सकती हूँ?\\n', '\\tयह लगभग सही है।\\n', '\\tवह तेल में पकाया गया है।\\n', '\\tवह तेल में पका हुआ है।\\n', '\\tयह एक बहुत अच्छा सवाल है।\\n', '\\tये अजीब है, नहीं?\\n', '\\tबच्चे ने रोना समाप्त कर दिया।\\n', '\\tबच्चे ने रोना बंद कर दिया।\\n', '\\tगेट को खुला छोड़ दिया गया था।\\n', '\\tउस लड़की ने पीछे मुड़कर देखा।\\n', '\\tयह चाकू तेज़ नहीं है।\\n', '\\tयह बत्ती काम नहीं करती।\\n', '\\tजितना जल्दी उतना अच्छा।\\n', '\\tवे बहुत प्रसन्न होंगे।\\n', '\\tयह किताब बहुत पतली है।\\n', '\\tयह शेर बहुत पालतू है।\\n', '\\tयह रेशम छूने में बहुत नर्म लगता है।\\n', '\\tटॉम ने इंजन को चालू कर दिया।\\n', '\\tहम अक्सर ग़लतियाँ कर बैठते हैं।\\n', '\\tतुम्हें यह करने के लिए बताया गया था?\\n', '\\tतम्हे क्या करना अच्छा लगता है?\\n', '\\tआपको क्या करना अच्छा लगता है?\\n', '\\tकितने बजे खुलता है?\\n', '\\tतापमान क्या है?\\n', '\\tतुम्हारे ख़याल से वह कौन है?\\n', '\\tहकला क्यों रहे हो?\\n', '\\tहम समय में पहुँच जाएँगे क्या?\\n', '\\tतुम्हारे पास काफ़ी समय था।\\n', '\\tतुम्हारी याददाश्त अच्छी है।\\n', '\\tतुम्हे टॉम को रोकना ही होगा।\\n', '\\tकुरसियाँ काफ़ी हैं क्या?\\n', '\\tतुम मंगलवार को खाली हो क्या?\\n', '\\tमैं आपका पासपोर्ट देख सकता हूँ क्या?\\n', '\\tतुम अंग्रेज़ी नहीं बोल सकते हो क्या?\\n', '\\tआप टीवी देखतें हैं क्या?\\n', '\\tदूध जल्दी से खराब हो जाता है क्या?\\n', '\\tशराब की एक बूंद भी मत पियो।\\n', '\\tटिकट भूलना मत।\\n', '\\tग्लास रेत से बनाया जाता है।\\n', '\\tउसने अपने परिवार को छोड़ दिया।\\n', '\\tउसने अपने देश को धोखा दे दिया।\\n', '\\tउसे ज़ुकाम बहुत आसानी से हो जाता है।\\n', '\\tउसको नौकरी नहीं मिल पायी।\\n', '\\tउसने विदेश जाने का निर्णय लिया।\\n', '\\tवह जल्दी नहीं उठा।\\n', '\\tवह पार्क में खो गया।\\n', '\\tवह सूप को सूँघ रहा है।\\n', '\\tउसको मेहनत करने की आदत है।\\n', '\\tउसको फ़ुटबॉल खेलना अच्छा लगता है।\\n', '\\tउसने आसमान की तरफ़ देखा।\\n', '\\tउसको घमण्डी होने की आदत है।\\n', '\\tउसको अहंकारी होने की आदत है।\\n', '\\tउसको अभिमानी होने की आदत है।\\n', '\\tउसपर चोरी का इलज़ाम लगाया गया।\\n', '\\tवह एक राजा से बहुत ज़्यादा था।\\n', '\\tउसकी कहानी सच्ची नहीं हो सकती।\\n', '\\tमेरे पास ज़्यादा समय नहीं है।\\n', '\\tमेरे पास ज़्यादा पैसे नहीं हैं।\\n', '\\tमुझे उल्टी आ रही है।\\n', '\\tमुझे उससे प्यार हो गया।\\n', '\\tमुझे गिलास खाली मिला।\\n', '\\tअब मेरे पास बहुत थोड़े पैसे हैं।\\n', '\\tमेरे पास अभी कोई पैसे नहीं हैं।\\n', '\\tमुझे दंत चिकित्सक से मिलना है।\\n', '\\tमुझे अपना काम बहुत पसंद है।\\n', '\\tमुझे अंग्रेज़ी पढ़ना अच्छा लगता है।\\n', '\\tमुझे शहर की ज़िन्दगी पसंद है।\\n', '\\tकाश मैं तुम्हारी मदद कर सकता।\\n', '\\tहो सके तो मैं उससे मिलना नहीं चाहूँगा।\\n', '\\tहो सके तो मैं उससे मिलना नहीं चाहूँगी।\\n', '\\tमैं तुम्हें एक तोहफ़ा दूँगा।\\n', '\\tमैंने बीयर पीना छोड़ दिया है।\\n', '\\tतुम यही चाहते थे ना?\\n', '\\tबारह बजने वाले हैं।\\n', '\\tछः बजने वाले हैं।\\n', '\\tइससे मेरे कोई लेना-देना नहीं है।\\n', '\\tमेरी बहन की टांगें बहुत लम्बीं हैं।\\n', '\\tमैं आपके लिए कुरसी लेआऊँ?\\n', '\\tउसने एक दर्जन अंडे खरीदे।\\n', '\\tवह मुझसे बात नहीं करती।\\n', '\\tउसने सफ़ेद कपड़े पहने हुएँ हैं।\\n', '\\tवह एक बार फिर देर से आई।\\n', '\\tमुझे एक और उदाहरण दो।\\n', '\\tमेरे लिए तैरना आसान है।\\n', '\\tकोईसा भी ले लो।\\n', '\\tयह उसपर भी लागु होता है।\\n', '\\tइस आदमी के पास बहुत सारे कर्ज़ हैं।\\n', '\\tडब्बा लकड़ी का बना है।\\n', '\\tबस अभी तक आई नहीं है।\\n', '\\tघर आग में लिपटा हुआ था।\\n', '\\tउन्होंने अनाथ बच्चे को गोद ले लिया।\\n', '\\tवे आमने-सामने खड़े हुए।\\n', '\\tवे तुमसे डरते थे।\\n', '\\tउन्हें तुमसे डर लगता था।\\n', '\\tयह कुरसी बहुत छोटी है।\\n', '\\tमैं इसीलिए वापस आया था।\\n', '\\tयह नदी बहुत सुन्दर है।\\n', '\\tकल मेरा जन्मदिन है।\\n', '\\tअपना मूँह इस ओर मोड़ो।\\n', '\\tअपना मूँह इस तरफ़ मोड़ो।\\n', '\\tदो और दो चार होते हैं।\\n', '\\tपानी बहुत जरूरी है।\\n', '\\tहम सब एक-साथ खड़े हो गए।\\n', '\\tहमने गोल मेज़ खरीदी।\\n', '\\tहमारी आज स्कूल से छुट्टी है।\\n', '\\tहम कल शायद नहीं जीतेंगे।\\n', '\\tहम चाय में चीनी डालते हैं।\\n', '\\tतुम्हे किसलिए चाहिए?\\n', '\\tउसकी राष्ट्रीयता क्या है?\\n', '\\tतुम टेनिस कब खेलना चाहते हो?\\n', '\\tजब बारिश होती है, तो ज़बरदस्त होती है।\\n', '\\tतुम कभी भी मदद क्यों नहीं करते?\\n', '\\tतू कभी भी मदद क्यों नहीं करता है?\\n', '\\tटीवी चालू करदोगे क्या?\\n', '\\tतुम्हें यहाँ इंतेज़ार नहीं करना चाहिए।\\n', '\\tबुरी खबर तेज़ी से फैलती है।\\n', '\\tक्या तुम इसे राज़ रख सकते हो?\\n', '\\tचीज़ दूध से बनता है।\\n', '\\tइन दोनों में से किसी एक को चुनो।\\n', '\\tतुमने अपना होमवर्क कर लिया क्या?\\n', '\\tहोमवर्क किया क्या?\\n', '\\tतुम्हारे पास कलम है क्या?\\n', '\\tतुम्हारे कोई भाई हैं क्या?\\n', '\\tआपको ऐमब्यूलेंस की ज़रूरत है क्या?\\n', '\\tउस चाबी के साथतानी मत करो!\\n', '\\tबाहर नहीं जाना चाहते हो क्या?\\n', '\\tहर घर में बगीचा था।\\n', '\\tवह कम-से-कम तीस साल का तो होगा।\\n', '\\tउसने लिफ़ाफ़े को काटकर खोल दिया।\\n', '\\tवह कैंसर से नहीं मरा।\\n', '\\tवह संगीत का बहुत शौकीन है।\\n', '\\tवह बहुत सारे लोगों को जानता है।\\n', '\\tउसने अपनी बंदूक का निशाना मुझपर लगाया।\\n', '\\tउसपर कत्ल का इलज़ाम लगाया गया।\\n', '\\tवह दस साल का लड़का था।\\n', '\\tवह मेरी बात नहीं सुनेगा।\\n', '\\tवह कभी भी संतुष्ट नहीं होता।\\n', '\\tवह अभी एक उपन्यास पढ़ रहा है।\\n', '\\tयहां, बैठिये।\\n', '\\tतुम कितनी देर से प्रतीक्षा कर रहे हो?\\n', '\\tआधा किलो कितने का है?\\n', '\\tमैं गाड़ी चला सकता हूँ।\\n', '\\tमाफ़ किजिए, पर मैं आपसे सहमत नहीं हूँ।\\n', '\\tमैं तुम्हारे बिना जी नहीं सकता।\\n', '\\tमैं तुम्हारे बगैर जी नहीं सकती।\\n']\n"
          ]
        }
      ],
      "source": [
        "print(target_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8Den3AOphHv"
      },
      "outputs": [],
      "source": [
        "input_characters=sorted(list(input_characters))\n",
        "target_characters=sorted(list(target_characters))\n",
        "\n",
        "num_encoder_tokens=len(input_characters)\n",
        "num_decoder_tokens=len(target_characters)\n",
        "\n",
        "max_encoder_seq_length=max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length=max([len(txt) for txt in target_texts])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjN7Owa2phEr",
        "outputId": "f808a2b9-ae26-48da-d072-bf52a6579d57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples: 900\n",
            "Number of unique input tokens: 67\n",
            "Number of unique output tokens: 77\n",
            "Max sequence length for inputs: 25\n",
            "Max sequence length for outputs: 60\n"
          ]
        }
      ],
      "source": [
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZIprFZKphCu"
      },
      "outputs": [],
      "source": [
        "input_token_index=dict(\n",
        "    [(char,i) for i, char in enumerate(input_characters)])\n",
        "target_token_index=dict(\n",
        "[(char,i) for i, char in enumerate(target_characters)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ysd5dMIbphAb"
      },
      "outputs": [],
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSli9wJMpg-R"
      },
      "outputs": [],
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
        "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp-KZuxGpg8M"
      },
      "outputs": [],
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGefiNQEpg4b",
        "outputId": "38e2ee46-2d54-4f5b-b83c-7b2bb12a9246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.8764 - loss: 0.4452 - val_accuracy: 0.7934 - val_loss: 0.7811\n",
            "Epoch 2/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8785 - loss: 0.4339 - val_accuracy: 0.7918 - val_loss: 0.7875\n",
            "Epoch 3/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8779 - loss: 0.4398 - val_accuracy: 0.7919 - val_loss: 0.7850\n",
            "Epoch 4/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8754 - loss: 0.4430 - val_accuracy: 0.7912 - val_loss: 0.7905\n",
            "Epoch 5/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8777 - loss: 0.4392 - val_accuracy: 0.7901 - val_loss: 0.7897\n",
            "Epoch 6/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8820 - loss: 0.4244 - val_accuracy: 0.7918 - val_loss: 0.7895\n",
            "Epoch 7/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8783 - loss: 0.4367 - val_accuracy: 0.7900 - val_loss: 0.7912\n",
            "Epoch 8/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8785 - loss: 0.4385 - val_accuracy: 0.7903 - val_loss: 0.7932\n",
            "Epoch 9/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8820 - loss: 0.4241 - val_accuracy: 0.7897 - val_loss: 0.7943\n",
            "Epoch 10/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8843 - loss: 0.4181 - val_accuracy: 0.7899 - val_loss: 0.7961\n",
            "Epoch 11/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8821 - loss: 0.4221 - val_accuracy: 0.7894 - val_loss: 0.7995\n",
            "Epoch 12/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8825 - loss: 0.4234 - val_accuracy: 0.7900 - val_loss: 0.7976\n",
            "Epoch 13/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8820 - loss: 0.4236 - val_accuracy: 0.7889 - val_loss: 0.8042\n",
            "Epoch 14/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8834 - loss: 0.4174 - val_accuracy: 0.7888 - val_loss: 0.8043\n",
            "Epoch 15/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8848 - loss: 0.4161 - val_accuracy: 0.7884 - val_loss: 0.8065\n",
            "Epoch 16/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8830 - loss: 0.4224 - val_accuracy: 0.7880 - val_loss: 0.8065\n",
            "Epoch 17/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8846 - loss: 0.4146 - val_accuracy: 0.7880 - val_loss: 0.8092\n",
            "Epoch 18/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8863 - loss: 0.4115 - val_accuracy: 0.7882 - val_loss: 0.8078\n",
            "Epoch 19/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8826 - loss: 0.4195 - val_accuracy: 0.7862 - val_loss: 0.8185\n",
            "Epoch 20/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8880 - loss: 0.4077 - val_accuracy: 0.7866 - val_loss: 0.8122\n",
            "Epoch 21/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8865 - loss: 0.4124 - val_accuracy: 0.7856 - val_loss: 0.8157\n",
            "Epoch 22/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8878 - loss: 0.4095 - val_accuracy: 0.7851 - val_loss: 0.8175\n",
            "Epoch 23/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8878 - loss: 0.4068 - val_accuracy: 0.7860 - val_loss: 0.8160\n",
            "Epoch 24/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8864 - loss: 0.4063 - val_accuracy: 0.7856 - val_loss: 0.8183\n",
            "Epoch 25/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8875 - loss: 0.4088 - val_accuracy: 0.7841 - val_loss: 0.8219\n",
            "Epoch 26/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8892 - loss: 0.4021 - val_accuracy: 0.7850 - val_loss: 0.8214\n",
            "Epoch 27/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8887 - loss: 0.4013 - val_accuracy: 0.7853 - val_loss: 0.8236\n",
            "Epoch 28/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8878 - loss: 0.4055 - val_accuracy: 0.7838 - val_loss: 0.8275\n",
            "Epoch 29/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8886 - loss: 0.3969 - val_accuracy: 0.7836 - val_loss: 0.8248\n",
            "Epoch 30/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8901 - loss: 0.3967 - val_accuracy: 0.7838 - val_loss: 0.8304\n",
            "Epoch 31/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8893 - loss: 0.3977 - val_accuracy: 0.7830 - val_loss: 0.8270\n",
            "Epoch 32/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8909 - loss: 0.3955 - val_accuracy: 0.7828 - val_loss: 0.8333\n",
            "Epoch 33/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8890 - loss: 0.3995 - val_accuracy: 0.7836 - val_loss: 0.8273\n",
            "Epoch 34/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8906 - loss: 0.3914 - val_accuracy: 0.7828 - val_loss: 0.8334\n",
            "Epoch 35/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8919 - loss: 0.3916 - val_accuracy: 0.7833 - val_loss: 0.8325\n",
            "Epoch 36/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8898 - loss: 0.3941 - val_accuracy: 0.7821 - val_loss: 0.8356\n",
            "Epoch 37/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8895 - loss: 0.3938 - val_accuracy: 0.7817 - val_loss: 0.8377\n",
            "Epoch 38/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8904 - loss: 0.3957 - val_accuracy: 0.7821 - val_loss: 0.8395\n",
            "Epoch 39/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8908 - loss: 0.3932 - val_accuracy: 0.7818 - val_loss: 0.8401\n",
            "Epoch 40/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8905 - loss: 0.3948 - val_accuracy: 0.7822 - val_loss: 0.8394\n",
            "Epoch 41/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8933 - loss: 0.3850 - val_accuracy: 0.7805 - val_loss: 0.8467\n",
            "Epoch 42/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8953 - loss: 0.3785 - val_accuracy: 0.7815 - val_loss: 0.8458\n",
            "Epoch 43/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8924 - loss: 0.3853 - val_accuracy: 0.7808 - val_loss: 0.8448\n",
            "Epoch 44/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8961 - loss: 0.3808 - val_accuracy: 0.7810 - val_loss: 0.8460\n",
            "Epoch 45/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8958 - loss: 0.3776 - val_accuracy: 0.7805 - val_loss: 0.8571\n",
            "Epoch 46/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8958 - loss: 0.3751 - val_accuracy: 0.7780 - val_loss: 0.8682\n",
            "Epoch 47/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8941 - loss: 0.3837 - val_accuracy: 0.7799 - val_loss: 0.8525\n",
            "Epoch 48/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8933 - loss: 0.3794 - val_accuracy: 0.7792 - val_loss: 0.8503\n",
            "Epoch 49/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8946 - loss: 0.3755 - val_accuracy: 0.7782 - val_loss: 0.8611\n",
            "Epoch 50/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8971 - loss: 0.3728 - val_accuracy: 0.7788 - val_loss: 0.8530\n",
            "Epoch 51/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8935 - loss: 0.3838 - val_accuracy: 0.7792 - val_loss: 0.8652\n",
            "Epoch 52/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8950 - loss: 0.3772 - val_accuracy: 0.7802 - val_loss: 0.8550\n",
            "Epoch 53/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8951 - loss: 0.3773 - val_accuracy: 0.7768 - val_loss: 0.8690\n",
            "Epoch 54/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8938 - loss: 0.3790 - val_accuracy: 0.7775 - val_loss: 0.8627\n",
            "Epoch 55/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8959 - loss: 0.3736 - val_accuracy: 0.7786 - val_loss: 0.8651\n",
            "Epoch 56/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8976 - loss: 0.3669 - val_accuracy: 0.7778 - val_loss: 0.8642\n",
            "Epoch 57/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8981 - loss: 0.3661 - val_accuracy: 0.7787 - val_loss: 0.8637\n",
            "Epoch 58/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8949 - loss: 0.3774 - val_accuracy: 0.7782 - val_loss: 0.8667\n",
            "Epoch 59/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8962 - loss: 0.3716 - val_accuracy: 0.7773 - val_loss: 0.8717\n",
            "Epoch 60/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8975 - loss: 0.3691 - val_accuracy: 0.7761 - val_loss: 0.8672\n",
            "Epoch 61/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8954 - loss: 0.3759 - val_accuracy: 0.7759 - val_loss: 0.8736\n",
            "Epoch 62/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8981 - loss: 0.3632 - val_accuracy: 0.7765 - val_loss: 0.8771\n",
            "Epoch 63/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8971 - loss: 0.3652 - val_accuracy: 0.7768 - val_loss: 0.8713\n",
            "Epoch 64/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8988 - loss: 0.3612 - val_accuracy: 0.7774 - val_loss: 0.8791\n",
            "Epoch 65/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9001 - loss: 0.3584 - val_accuracy: 0.7761 - val_loss: 0.8777\n",
            "Epoch 66/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9009 - loss: 0.3586 - val_accuracy: 0.7768 - val_loss: 0.8801\n",
            "Epoch 67/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8999 - loss: 0.3568 - val_accuracy: 0.7769 - val_loss: 0.8770\n",
            "Epoch 68/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8989 - loss: 0.3644 - val_accuracy: 0.7753 - val_loss: 0.8861\n",
            "Epoch 69/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9001 - loss: 0.3564 - val_accuracy: 0.7753 - val_loss: 0.8806\n",
            "Epoch 70/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9011 - loss: 0.3546 - val_accuracy: 0.7769 - val_loss: 0.8907\n",
            "Epoch 71/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9003 - loss: 0.3554 - val_accuracy: 0.7747 - val_loss: 0.8866\n",
            "Epoch 72/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9010 - loss: 0.3571 - val_accuracy: 0.7758 - val_loss: 0.8860\n",
            "Epoch 73/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9023 - loss: 0.3536 - val_accuracy: 0.7740 - val_loss: 0.8925\n",
            "Epoch 74/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9027 - loss: 0.3511 - val_accuracy: 0.7749 - val_loss: 0.8872\n",
            "Epoch 75/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8995 - loss: 0.3576 - val_accuracy: 0.7735 - val_loss: 0.8964\n",
            "Epoch 76/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9025 - loss: 0.3507 - val_accuracy: 0.7738 - val_loss: 0.8941\n",
            "Epoch 77/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9029 - loss: 0.3510 - val_accuracy: 0.7725 - val_loss: 0.9050\n",
            "Epoch 78/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9031 - loss: 0.3515 - val_accuracy: 0.7738 - val_loss: 0.8974\n",
            "Epoch 79/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9064 - loss: 0.3395 - val_accuracy: 0.7735 - val_loss: 0.9016\n",
            "Epoch 80/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9030 - loss: 0.3512 - val_accuracy: 0.7731 - val_loss: 0.9004\n",
            "Epoch 81/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9053 - loss: 0.3431 - val_accuracy: 0.7735 - val_loss: 0.9023\n",
            "Epoch 82/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9046 - loss: 0.3444 - val_accuracy: 0.7728 - val_loss: 0.9116\n",
            "Epoch 83/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9047 - loss: 0.3478 - val_accuracy: 0.7737 - val_loss: 0.9049\n",
            "Epoch 84/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9063 - loss: 0.3396 - val_accuracy: 0.7736 - val_loss: 0.9094\n",
            "Epoch 85/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9065 - loss: 0.3400 - val_accuracy: 0.7730 - val_loss: 0.9060\n",
            "Epoch 86/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9059 - loss: 0.3421 - val_accuracy: 0.7724 - val_loss: 0.9134\n",
            "Epoch 87/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9073 - loss: 0.3339 - val_accuracy: 0.7727 - val_loss: 0.9096\n",
            "Epoch 88/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9069 - loss: 0.3385 - val_accuracy: 0.7714 - val_loss: 0.9190\n",
            "Epoch 89/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9065 - loss: 0.3423 - val_accuracy: 0.7718 - val_loss: 0.9188\n",
            "Epoch 90/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9076 - loss: 0.3362 - val_accuracy: 0.7719 - val_loss: 0.9220\n",
            "Epoch 91/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9084 - loss: 0.3327 - val_accuracy: 0.7718 - val_loss: 0.9175\n",
            "Epoch 92/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9073 - loss: 0.3359 - val_accuracy: 0.7719 - val_loss: 0.9143\n",
            "Epoch 93/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9067 - loss: 0.3343 - val_accuracy: 0.7721 - val_loss: 0.9311\n",
            "Epoch 94/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9071 - loss: 0.3349 - val_accuracy: 0.7707 - val_loss: 0.9178\n",
            "Epoch 95/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9092 - loss: 0.3341 - val_accuracy: 0.7716 - val_loss: 0.9324\n",
            "Epoch 96/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9087 - loss: 0.3314 - val_accuracy: 0.7718 - val_loss: 0.9244\n",
            "Epoch 97/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9062 - loss: 0.3361 - val_accuracy: 0.7683 - val_loss: 0.9458\n",
            "Epoch 98/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9081 - loss: 0.3289 - val_accuracy: 0.7694 - val_loss: 0.9390\n",
            "Epoch 99/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9113 - loss: 0.3221 - val_accuracy: 0.7685 - val_loss: 0.9463\n",
            "Epoch 100/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9109 - loss: 0.3230 - val_accuracy: 0.7680 - val_loss: 0.9465\n",
            "Epoch 101/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9114 - loss: 0.3268 - val_accuracy: 0.7703 - val_loss: 0.9383\n",
            "Epoch 102/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9105 - loss: 0.3233 - val_accuracy: 0.7669 - val_loss: 0.9524\n",
            "Epoch 103/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9131 - loss: 0.3191 - val_accuracy: 0.7681 - val_loss: 0.9507\n",
            "Epoch 104/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9106 - loss: 0.3264 - val_accuracy: 0.7678 - val_loss: 0.9505\n",
            "Epoch 105/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9090 - loss: 0.3295 - val_accuracy: 0.7681 - val_loss: 0.9495\n",
            "Epoch 106/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9124 - loss: 0.3159 - val_accuracy: 0.7695 - val_loss: 0.9417\n",
            "Epoch 107/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9122 - loss: 0.3172 - val_accuracy: 0.7675 - val_loss: 0.9508\n",
            "Epoch 108/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9116 - loss: 0.3203 - val_accuracy: 0.7692 - val_loss: 0.9482\n",
            "Epoch 109/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9120 - loss: 0.3206 - val_accuracy: 0.7675 - val_loss: 0.9555\n",
            "Epoch 110/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9139 - loss: 0.3156 - val_accuracy: 0.7668 - val_loss: 0.9656\n",
            "Epoch 111/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9120 - loss: 0.3156 - val_accuracy: 0.7675 - val_loss: 0.9500\n",
            "Epoch 112/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9133 - loss: 0.3135 - val_accuracy: 0.7660 - val_loss: 0.9616\n",
            "Epoch 113/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9122 - loss: 0.3195 - val_accuracy: 0.7675 - val_loss: 0.9673\n",
            "Epoch 114/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9126 - loss: 0.3166 - val_accuracy: 0.7668 - val_loss: 0.9635\n",
            "Epoch 115/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9133 - loss: 0.3125 - val_accuracy: 0.7664 - val_loss: 0.9610\n",
            "Epoch 116/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9127 - loss: 0.3169 - val_accuracy: 0.7653 - val_loss: 0.9675\n",
            "Epoch 117/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9136 - loss: 0.3140 - val_accuracy: 0.7657 - val_loss: 0.9747\n",
            "Epoch 118/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9145 - loss: 0.3138 - val_accuracy: 0.7675 - val_loss: 0.9567\n",
            "Epoch 119/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9114 - loss: 0.3174 - val_accuracy: 0.7665 - val_loss: 0.9700\n",
            "Epoch 120/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9161 - loss: 0.3085 - val_accuracy: 0.7660 - val_loss: 0.9749\n",
            "Epoch 121/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9161 - loss: 0.3080 - val_accuracy: 0.7675 - val_loss: 0.9674\n",
            "Epoch 122/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9157 - loss: 0.3084 - val_accuracy: 0.7674 - val_loss: 0.9635\n",
            "Epoch 123/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9176 - loss: 0.3015 - val_accuracy: 0.7674 - val_loss: 0.9746\n",
            "Epoch 124/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9175 - loss: 0.3040 - val_accuracy: 0.7668 - val_loss: 0.9743\n",
            "Epoch 125/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9156 - loss: 0.3064 - val_accuracy: 0.7651 - val_loss: 0.9741\n",
            "Epoch 126/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9180 - loss: 0.3014 - val_accuracy: 0.7662 - val_loss: 0.9729\n",
            "Epoch 127/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9174 - loss: 0.3025 - val_accuracy: 0.7653 - val_loss: 0.9763\n",
            "Epoch 128/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9181 - loss: 0.2994 - val_accuracy: 0.7667 - val_loss: 0.9799\n",
            "Epoch 129/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9176 - loss: 0.2982 - val_accuracy: 0.7656 - val_loss: 0.9771\n",
            "Epoch 130/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9196 - loss: 0.2969 - val_accuracy: 0.7624 - val_loss: 0.9936\n",
            "Epoch 131/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9194 - loss: 0.2976 - val_accuracy: 0.7654 - val_loss: 0.9869\n",
            "Epoch 132/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9201 - loss: 0.2959 - val_accuracy: 0.7649 - val_loss: 0.9852\n",
            "Epoch 133/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9170 - loss: 0.3018 - val_accuracy: 0.7637 - val_loss: 0.9984\n",
            "Epoch 134/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9201 - loss: 0.2944 - val_accuracy: 0.7652 - val_loss: 0.9912\n",
            "Epoch 135/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9185 - loss: 0.2971 - val_accuracy: 0.7636 - val_loss: 0.9947\n",
            "Epoch 136/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9221 - loss: 0.2879 - val_accuracy: 0.7650 - val_loss: 0.9954\n",
            "Epoch 137/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9168 - loss: 0.2988 - val_accuracy: 0.7639 - val_loss: 0.9986\n",
            "Epoch 138/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9211 - loss: 0.2862 - val_accuracy: 0.7650 - val_loss: 0.9928\n",
            "Epoch 139/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9194 - loss: 0.2932 - val_accuracy: 0.7647 - val_loss: 0.9984\n",
            "Epoch 140/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9212 - loss: 0.2887 - val_accuracy: 0.7644 - val_loss: 1.0021\n",
            "Epoch 141/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9233 - loss: 0.2846 - val_accuracy: 0.7639 - val_loss: 1.0008\n",
            "Epoch 142/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9230 - loss: 0.2820 - val_accuracy: 0.7631 - val_loss: 1.0026\n",
            "Epoch 143/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9227 - loss: 0.2865 - val_accuracy: 0.7637 - val_loss: 1.0087\n",
            "Epoch 144/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9235 - loss: 0.2831 - val_accuracy: 0.7648 - val_loss: 1.0069\n",
            "Epoch 145/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9238 - loss: 0.2797 - val_accuracy: 0.7614 - val_loss: 1.0095\n",
            "Epoch 146/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9235 - loss: 0.2791 - val_accuracy: 0.7625 - val_loss: 1.0170\n",
            "Epoch 147/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9238 - loss: 0.2808 - val_accuracy: 0.7643 - val_loss: 1.0127\n",
            "Epoch 148/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9244 - loss: 0.2771 - val_accuracy: 0.7637 - val_loss: 1.0116\n",
            "Epoch 149/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9240 - loss: 0.2827 - val_accuracy: 0.7631 - val_loss: 1.0167\n",
            "Epoch 150/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9238 - loss: 0.2782 - val_accuracy: 0.7627 - val_loss: 1.0257\n",
            "Epoch 151/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9246 - loss: 0.2767 - val_accuracy: 0.7634 - val_loss: 1.0138\n",
            "Epoch 152/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9247 - loss: 0.2789 - val_accuracy: 0.7622 - val_loss: 1.0220\n",
            "Epoch 153/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9231 - loss: 0.2782 - val_accuracy: 0.7605 - val_loss: 1.0368\n",
            "Epoch 154/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9234 - loss: 0.2831 - val_accuracy: 0.7611 - val_loss: 1.0271\n",
            "Epoch 155/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9219 - loss: 0.2848 - val_accuracy: 0.7613 - val_loss: 1.0318\n",
            "Epoch 156/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9252 - loss: 0.2780 - val_accuracy: 0.7630 - val_loss: 1.0305\n",
            "Epoch 157/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9249 - loss: 0.2742 - val_accuracy: 0.7601 - val_loss: 1.0343\n",
            "Epoch 158/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9262 - loss: 0.2732 - val_accuracy: 0.7619 - val_loss: 1.0354\n",
            "Epoch 159/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9267 - loss: 0.2708 - val_accuracy: 0.7613 - val_loss: 1.0371\n",
            "Epoch 160/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9240 - loss: 0.2775 - val_accuracy: 0.7617 - val_loss: 1.0389\n",
            "Epoch 161/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9259 - loss: 0.2732 - val_accuracy: 0.7626 - val_loss: 1.0383\n",
            "Epoch 162/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9250 - loss: 0.2738 - val_accuracy: 0.7601 - val_loss: 1.0457\n",
            "Epoch 163/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9263 - loss: 0.2742 - val_accuracy: 0.7603 - val_loss: 1.0412\n",
            "Epoch 164/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9284 - loss: 0.2678 - val_accuracy: 0.7606 - val_loss: 1.0481\n",
            "Epoch 165/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9259 - loss: 0.2696 - val_accuracy: 0.7596 - val_loss: 1.0509\n",
            "Epoch 166/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9270 - loss: 0.2692 - val_accuracy: 0.7608 - val_loss: 1.0551\n",
            "Epoch 167/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9285 - loss: 0.2651 - val_accuracy: 0.7600 - val_loss: 1.0506\n",
            "Epoch 168/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9284 - loss: 0.2700 - val_accuracy: 0.7598 - val_loss: 1.0580\n",
            "Epoch 169/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9272 - loss: 0.2690 - val_accuracy: 0.7603 - val_loss: 1.0511\n",
            "Epoch 170/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9285 - loss: 0.2640 - val_accuracy: 0.7604 - val_loss: 1.0538\n",
            "Epoch 171/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9267 - loss: 0.2703 - val_accuracy: 0.7604 - val_loss: 1.0538\n",
            "Epoch 172/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9263 - loss: 0.2708 - val_accuracy: 0.7580 - val_loss: 1.0625\n",
            "Epoch 173/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9308 - loss: 0.2611 - val_accuracy: 0.7587 - val_loss: 1.0613\n",
            "Epoch 174/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9269 - loss: 0.2661 - val_accuracy: 0.7580 - val_loss: 1.0691\n",
            "Epoch 175/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9284 - loss: 0.2631 - val_accuracy: 0.7610 - val_loss: 1.0530\n",
            "Epoch 176/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9284 - loss: 0.2600 - val_accuracy: 0.7581 - val_loss: 1.0693\n",
            "Epoch 177/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9295 - loss: 0.2637 - val_accuracy: 0.7593 - val_loss: 1.0698\n",
            "Epoch 178/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9309 - loss: 0.2564 - val_accuracy: 0.7592 - val_loss: 1.0723\n",
            "Epoch 179/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9291 - loss: 0.2620 - val_accuracy: 0.7580 - val_loss: 1.0729\n",
            "Epoch 180/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9314 - loss: 0.2550 - val_accuracy: 0.7580 - val_loss: 1.0702\n",
            "Epoch 181/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9309 - loss: 0.2575 - val_accuracy: 0.7585 - val_loss: 1.0725\n",
            "Epoch 182/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9319 - loss: 0.2519 - val_accuracy: 0.7578 - val_loss: 1.0676\n",
            "Epoch 183/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9338 - loss: 0.2513 - val_accuracy: 0.7579 - val_loss: 1.0908\n",
            "Epoch 184/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9319 - loss: 0.2517 - val_accuracy: 0.7569 - val_loss: 1.0782\n",
            "Epoch 185/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9332 - loss: 0.2526 - val_accuracy: 0.7575 - val_loss: 1.0808\n",
            "Epoch 186/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9323 - loss: 0.2514 - val_accuracy: 0.7584 - val_loss: 1.0769\n",
            "Epoch 187/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9309 - loss: 0.2561 - val_accuracy: 0.7568 - val_loss: 1.0879\n",
            "Epoch 188/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9330 - loss: 0.2495 - val_accuracy: 0.7565 - val_loss: 1.0850\n",
            "Epoch 189/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9342 - loss: 0.2475 - val_accuracy: 0.7577 - val_loss: 1.0818\n",
            "Epoch 190/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9296 - loss: 0.2646 - val_accuracy: 0.7566 - val_loss: 1.0939\n",
            "Epoch 191/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9305 - loss: 0.2553 - val_accuracy: 0.7582 - val_loss: 1.1011\n",
            "Epoch 192/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9294 - loss: 0.2577 - val_accuracy: 0.7562 - val_loss: 1.0920\n",
            "Epoch 193/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9329 - loss: 0.2538 - val_accuracy: 0.7580 - val_loss: 1.0951\n",
            "Epoch 194/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9315 - loss: 0.2523 - val_accuracy: 0.7580 - val_loss: 1.0899\n",
            "Epoch 195/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9306 - loss: 0.2532 - val_accuracy: 0.7571 - val_loss: 1.0895\n",
            "Epoch 196/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9342 - loss: 0.2482 - val_accuracy: 0.7575 - val_loss: 1.0933\n",
            "Epoch 197/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9329 - loss: 0.2510 - val_accuracy: 0.7587 - val_loss: 1.0920\n",
            "Epoch 198/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9342 - loss: 0.2446 - val_accuracy: 0.7566 - val_loss: 1.0990\n",
            "Epoch 199/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9344 - loss: 0.2459 - val_accuracy: 0.7561 - val_loss: 1.1027\n",
            "Epoch 200/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9355 - loss: 0.2412 - val_accuracy: 0.7597 - val_loss: 1.0880\n",
            "Epoch 201/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9319 - loss: 0.2510 - val_accuracy: 0.7561 - val_loss: 1.1095\n",
            "Epoch 202/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9340 - loss: 0.2475 - val_accuracy: 0.7571 - val_loss: 1.0961\n",
            "Epoch 203/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9335 - loss: 0.2439 - val_accuracy: 0.7586 - val_loss: 1.1003\n",
            "Epoch 204/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9335 - loss: 0.2430 - val_accuracy: 0.7571 - val_loss: 1.0966\n",
            "Epoch 205/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9350 - loss: 0.2429 - val_accuracy: 0.7580 - val_loss: 1.1053\n",
            "Epoch 206/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9335 - loss: 0.2419 - val_accuracy: 0.7567 - val_loss: 1.1023\n",
            "Epoch 207/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9361 - loss: 0.2363 - val_accuracy: 0.7559 - val_loss: 1.1038\n",
            "Epoch 208/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9361 - loss: 0.2408 - val_accuracy: 0.7570 - val_loss: 1.1118\n",
            "Epoch 209/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9353 - loss: 0.2404 - val_accuracy: 0.7550 - val_loss: 1.1079\n",
            "Epoch 210/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9387 - loss: 0.2356 - val_accuracy: 0.7560 - val_loss: 1.1128\n",
            "Epoch 211/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9346 - loss: 0.2377 - val_accuracy: 0.7559 - val_loss: 1.1117\n",
            "Epoch 212/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9381 - loss: 0.2357 - val_accuracy: 0.7561 - val_loss: 1.1185\n",
            "Epoch 213/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9393 - loss: 0.2319 - val_accuracy: 0.7549 - val_loss: 1.1218\n",
            "Epoch 214/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9384 - loss: 0.2326 - val_accuracy: 0.7561 - val_loss: 1.1174\n",
            "Epoch 215/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9382 - loss: 0.2327 - val_accuracy: 0.7544 - val_loss: 1.1254\n",
            "Epoch 216/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9401 - loss: 0.2299 - val_accuracy: 0.7546 - val_loss: 1.1298\n",
            "Epoch 217/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9412 - loss: 0.2261 - val_accuracy: 0.7558 - val_loss: 1.1233\n",
            "Epoch 218/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9412 - loss: 0.2257 - val_accuracy: 0.7541 - val_loss: 1.1341\n",
            "Epoch 219/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9413 - loss: 0.2256 - val_accuracy: 0.7548 - val_loss: 1.1361\n",
            "Epoch 220/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9387 - loss: 0.2296 - val_accuracy: 0.7551 - val_loss: 1.1402\n",
            "Epoch 221/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9399 - loss: 0.2248 - val_accuracy: 0.7541 - val_loss: 1.1327\n",
            "Epoch 222/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9417 - loss: 0.2262 - val_accuracy: 0.7555 - val_loss: 1.1384\n",
            "Epoch 223/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9422 - loss: 0.2211 - val_accuracy: 0.7534 - val_loss: 1.1473\n",
            "Epoch 224/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9413 - loss: 0.2224 - val_accuracy: 0.7551 - val_loss: 1.1423\n",
            "Epoch 225/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9407 - loss: 0.2292 - val_accuracy: 0.7546 - val_loss: 1.1457\n",
            "Epoch 226/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9421 - loss: 0.2226 - val_accuracy: 0.7537 - val_loss: 1.1501\n",
            "Epoch 227/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9424 - loss: 0.2199 - val_accuracy: 0.7547 - val_loss: 1.1453\n",
            "Epoch 228/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9433 - loss: 0.2175 - val_accuracy: 0.7540 - val_loss: 1.1465\n",
            "Epoch 229/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9421 - loss: 0.2208 - val_accuracy: 0.7539 - val_loss: 1.1547\n",
            "Epoch 230/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9411 - loss: 0.2237 - val_accuracy: 0.7531 - val_loss: 1.1592\n",
            "Epoch 231/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9431 - loss: 0.2163 - val_accuracy: 0.7540 - val_loss: 1.1552\n",
            "Epoch 232/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9433 - loss: 0.2167 - val_accuracy: 0.7547 - val_loss: 1.1559\n",
            "Epoch 233/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9444 - loss: 0.2159 - val_accuracy: 0.7531 - val_loss: 1.1571\n",
            "Epoch 234/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9442 - loss: 0.2147 - val_accuracy: 0.7520 - val_loss: 1.1595\n",
            "Epoch 235/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9451 - loss: 0.2144 - val_accuracy: 0.7535 - val_loss: 1.1624\n",
            "Epoch 236/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9445 - loss: 0.2173 - val_accuracy: 0.7527 - val_loss: 1.1675\n",
            "Epoch 237/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9422 - loss: 0.2221 - val_accuracy: 0.7546 - val_loss: 1.1637\n",
            "Epoch 238/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9425 - loss: 0.2189 - val_accuracy: 0.7510 - val_loss: 1.1704\n",
            "Epoch 239/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9436 - loss: 0.2168 - val_accuracy: 0.7532 - val_loss: 1.1826\n",
            "Epoch 240/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9431 - loss: 0.2178 - val_accuracy: 0.7531 - val_loss: 1.1666\n",
            "Epoch 241/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9441 - loss: 0.2163 - val_accuracy: 0.7534 - val_loss: 1.1615\n",
            "Epoch 242/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9431 - loss: 0.2190 - val_accuracy: 0.7523 - val_loss: 1.1785\n",
            "Epoch 243/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9422 - loss: 0.2149 - val_accuracy: 0.7535 - val_loss: 1.1617\n",
            "Epoch 244/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9423 - loss: 0.2180 - val_accuracy: 0.7537 - val_loss: 1.1671\n",
            "Epoch 245/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9427 - loss: 0.2168 - val_accuracy: 0.7519 - val_loss: 1.1682\n",
            "Epoch 246/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9464 - loss: 0.2116 - val_accuracy: 0.7542 - val_loss: 1.1711\n",
            "Epoch 247/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9440 - loss: 0.2118 - val_accuracy: 0.7535 - val_loss: 1.1721\n",
            "Epoch 248/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9445 - loss: 0.2154 - val_accuracy: 0.7516 - val_loss: 1.1787\n",
            "Epoch 249/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9434 - loss: 0.2154 - val_accuracy: 0.7552 - val_loss: 1.1734\n",
            "Epoch 250/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9453 - loss: 0.2077 - val_accuracy: 0.7524 - val_loss: 1.1796\n",
            "Epoch 251/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9450 - loss: 0.2111 - val_accuracy: 0.7539 - val_loss: 1.1771\n",
            "Epoch 252/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9474 - loss: 0.2042 - val_accuracy: 0.7512 - val_loss: 1.1859\n",
            "Epoch 253/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9478 - loss: 0.2029 - val_accuracy: 0.7515 - val_loss: 1.1946\n",
            "Epoch 254/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9475 - loss: 0.2039 - val_accuracy: 0.7518 - val_loss: 1.1888\n",
            "Epoch 255/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9466 - loss: 0.2082 - val_accuracy: 0.7519 - val_loss: 1.1840\n",
            "Epoch 256/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9474 - loss: 0.2059 - val_accuracy: 0.7523 - val_loss: 1.1937\n",
            "Epoch 257/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9484 - loss: 0.2042 - val_accuracy: 0.7508 - val_loss: 1.1962\n",
            "Epoch 258/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9484 - loss: 0.2031 - val_accuracy: 0.7518 - val_loss: 1.2052\n",
            "Epoch 259/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9492 - loss: 0.1984 - val_accuracy: 0.7510 - val_loss: 1.2047\n",
            "Epoch 260/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9485 - loss: 0.1993 - val_accuracy: 0.7506 - val_loss: 1.2074\n",
            "Epoch 261/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9484 - loss: 0.2025 - val_accuracy: 0.7513 - val_loss: 1.2043\n",
            "Epoch 262/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9496 - loss: 0.2000 - val_accuracy: 0.7522 - val_loss: 1.2079\n",
            "Epoch 263/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9494 - loss: 0.2017 - val_accuracy: 0.7508 - val_loss: 1.2065\n",
            "Epoch 264/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9477 - loss: 0.2024 - val_accuracy: 0.7504 - val_loss: 1.2202\n",
            "Epoch 265/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9488 - loss: 0.2003 - val_accuracy: 0.7514 - val_loss: 1.2139\n",
            "Epoch 266/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9516 - loss: 0.1957 - val_accuracy: 0.7513 - val_loss: 1.2128\n",
            "Epoch 267/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9486 - loss: 0.1966 - val_accuracy: 0.7478 - val_loss: 1.2250\n",
            "Epoch 268/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9505 - loss: 0.1982 - val_accuracy: 0.7501 - val_loss: 1.2226\n",
            "Epoch 269/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9514 - loss: 0.1915 - val_accuracy: 0.7502 - val_loss: 1.2209\n",
            "Epoch 270/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9525 - loss: 0.1915 - val_accuracy: 0.7487 - val_loss: 1.2322\n",
            "Epoch 271/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9496 - loss: 0.1959 - val_accuracy: 0.7494 - val_loss: 1.2291\n",
            "Epoch 272/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9516 - loss: 0.1922 - val_accuracy: 0.7496 - val_loss: 1.2329\n",
            "Epoch 273/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9518 - loss: 0.1902 - val_accuracy: 0.7488 - val_loss: 1.2328\n",
            "Epoch 274/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9510 - loss: 0.1932 - val_accuracy: 0.7490 - val_loss: 1.2422\n",
            "Epoch 275/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9506 - loss: 0.1923 - val_accuracy: 0.7496 - val_loss: 1.2383\n",
            "Epoch 276/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9512 - loss: 0.1927 - val_accuracy: 0.7520 - val_loss: 1.2398\n",
            "Epoch 277/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9484 - loss: 0.2006 - val_accuracy: 0.7487 - val_loss: 1.2302\n",
            "Epoch 278/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9517 - loss: 0.1931 - val_accuracy: 0.7503 - val_loss: 1.2357\n",
            "Epoch 279/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9488 - loss: 0.1976 - val_accuracy: 0.7483 - val_loss: 1.2397\n",
            "Epoch 280/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9521 - loss: 0.1919 - val_accuracy: 0.7489 - val_loss: 1.2379\n",
            "Epoch 281/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9506 - loss: 0.1920 - val_accuracy: 0.7486 - val_loss: 1.2362\n",
            "Epoch 282/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9501 - loss: 0.1903 - val_accuracy: 0.7496 - val_loss: 1.2393\n",
            "Epoch 283/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9470 - loss: 0.1985 - val_accuracy: 0.7486 - val_loss: 1.2337\n",
            "Epoch 284/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9488 - loss: 0.1967 - val_accuracy: 0.7497 - val_loss: 1.2406\n",
            "Epoch 285/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9479 - loss: 0.1957 - val_accuracy: 0.7513 - val_loss: 1.2388\n",
            "Epoch 286/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9446 - loss: 0.2053 - val_accuracy: 0.7475 - val_loss: 1.2418\n",
            "Epoch 287/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9489 - loss: 0.1966 - val_accuracy: 0.7494 - val_loss: 1.2331\n",
            "Epoch 288/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9486 - loss: 0.1963 - val_accuracy: 0.7477 - val_loss: 1.2527\n",
            "Epoch 289/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9504 - loss: 0.1900 - val_accuracy: 0.7496 - val_loss: 1.2372\n",
            "Epoch 290/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9481 - loss: 0.1963 - val_accuracy: 0.7504 - val_loss: 1.2381\n",
            "Epoch 291/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9512 - loss: 0.1886 - val_accuracy: 0.7504 - val_loss: 1.2489\n",
            "Epoch 292/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9489 - loss: 0.1945 - val_accuracy: 0.7514 - val_loss: 1.2344\n",
            "Epoch 293/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9525 - loss: 0.1861 - val_accuracy: 0.7499 - val_loss: 1.2423\n",
            "Epoch 294/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9517 - loss: 0.1878 - val_accuracy: 0.7498 - val_loss: 1.2373\n",
            "Epoch 295/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9532 - loss: 0.1815 - val_accuracy: 0.7493 - val_loss: 1.2435\n",
            "Epoch 296/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9535 - loss: 0.1803 - val_accuracy: 0.7492 - val_loss: 1.2384\n",
            "Epoch 297/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9540 - loss: 0.1825 - val_accuracy: 0.7484 - val_loss: 1.2565\n",
            "Epoch 298/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9527 - loss: 0.1869 - val_accuracy: 0.7491 - val_loss: 1.2534\n",
            "Epoch 299/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9534 - loss: 0.1844 - val_accuracy: 0.7494 - val_loss: 1.2535\n",
            "Epoch 300/300\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9544 - loss: 0.1811 - val_accuracy: 0.7484 - val_loss: 1.2509\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7dbe88b46f10>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the model that will turn using adam optimizer & split 0.4\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model1 = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Run training\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model1.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CALeo8KtUA2",
        "outputId": "f48739d6-d9d4-4c85-e808-b0d4e0d5d592"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model1.save('hin2eng1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0jAEOlEpg2g"
      },
      "outputs": [],
      "source": [
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjOIUsF7pg0Z"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "         # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IqJg86TwkZd",
        "outputId": "70fb3136-b666-450b-e1aa-70a7503ebbf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "900"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(input_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQp0n615vFAh",
        "outputId": "d21e196f-c69f-4c51-9c5e-9b96fa523116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: शाबा!\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: बचाओ!\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: उछलो.\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: नमस्कार।\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: बाहु\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: समझे की रहा?\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "-\n",
            "Input sentence: I'm OK.\n",
            "Decoded sentence: मैं ठीक हूँ।\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "-\n",
            "Input sentence: Awesome!\n",
            "Decoded sentence: बहुत बढ़िया!\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "-\n",
            "Input sentence: Come in.\n",
            "Decoded sentence: अंदर आ जाओ।\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "-\n",
            "Input sentence: Get out!\n",
            "Decoded sentence: बाहर निकल जाओ!\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "-\n",
            "Input sentence: Go away!\n",
            "Decoded sentence: चलो ज़!\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "-\n",
            "Input sentence: Goodbye!\n",
            "Decoded sentence: शाबाश!\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "-\n",
            "Input sentence: Perfect!\n",
            "Decoded sentence: शाबाश!\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "-\n",
            "Input sentence: Welcome.\n",
            "Decoded sentence: सूल  खो।\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "-\n",
            "Input sentence: Have fun.\n",
            "Decoded sentence: मज़े करो।\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "-\n",
            "Input sentence: I forgot.\n",
            "Decoded sentence: मैं भूल गया।\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "-\n",
            "Input sentence: I'll pay.\n",
            "Decoded sentence: मैं आपसे पानता।\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "-\n",
            "Input sentence: I'm fine.\n",
            "Decoded sentence: मैं ठीक हूँ।\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "-\n",
            "Input sentence: I'm full.\n",
            "Decoded sentence: मेरा लिया।\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "-\n",
            "Input sentence: Let's go!\n",
            "Decoded sentence: चलो चलें!\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "-\n",
            "Input sentence: Answer me.\n",
            "Decoded sentence: मुझे जिखानाँगा।\n",
            "\n"
          ]
        }
      ],
      "source": [
        "seen_sentences = set()\n",
        "\n",
        "for seq_index in range(30):\n",
        "    input_sentence = input_texts[seq_index]\n",
        "\n",
        "    if input_sentence in seen_sentences:\n",
        "        continue  # Skip if already decoded\n",
        "    seen_sentences.add(input_sentence)\n",
        "\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "    print('-')\n",
        "    print('Input sentence:', input_sentence)\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz1gigzovEwX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
